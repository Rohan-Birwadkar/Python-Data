{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all the quotes and their author names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = open('quotes_authors',mode = 'w',newline = '',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_writer = csv.writer(QA,delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_writer.writerow(['The_Quote','The_Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting soup object and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = \"https://quotes.toscrape.com/page/{}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(main_url.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the grabbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'change'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('.author')[0].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change\n",
      "deep-thoughts\n",
      "thinking\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "soup.select('.tag')[0].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = soup.select('.quote')[0].select('.text')[0].getText()\n",
    "q = q.encode('ascii','ignore')\n",
    "q = q.decode()\n",
    "\n",
    "a = soup.select('.author')[0].getText()\n",
    "a = a.encode('ascii','ignore')\n",
    "a = a.decode()\n",
    "\n",
    "t= soup.select('.tags')[0].getText()\n",
    "t = t.strip()\n",
    "t = t.encode('ascii','ignore')\n",
    "t = t.decode()\n",
    " \n",
    "print(q,a,t[19:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('.quote')[9].select('.text')[0].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integrating the codes to get Quotes and Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to extract quotes and its authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached final page ending program\n"
     ]
    }
   ],
   "source": [
    "page_number = 0\n",
    "q = 'a'\n",
    "a = 'q'\n",
    "qa_list = []\n",
    "while True:\n",
    "    page_number+=1\n",
    "    res = requests.get(main_url.format(page_number))\n",
    "    soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "    if \"No quotes found!\" in soup.select('.col-md-8')[1].getText():\n",
    "        print(\"reached final page ending program\")\n",
    "        break\n",
    "    for i in range(0,10):\n",
    "        if \"No quotes found!\" in soup.select('.col-md-8')[1].getText():\n",
    "            print(\"reached final page ending program\")\n",
    "            break\n",
    "        q = soup.select('.quote')[i].select('.text')[0].getText()\n",
    "        q = q.encode('ascii','ignore')\n",
    "        q = q.decode()\n",
    "        \n",
    "        a = soup.select('.author')[i].getText()\n",
    "        a = a.encode('ascii','ignore')\n",
    "        a = a.decode()\n",
    "        csv_writer.writerow([q,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ROHAN\\\\Desktop\\\\All Is Here\\\\course work\\\\pyton'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Tags for the quote and names of their Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA = open('tags_authors',mode = 'w',newline = '',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_writer = csv.writer(TA,delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = \"https://quotes.toscrape.com/page/{}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached final page ending program\n"
     ]
    }
   ],
   "source": [
    "page_number = 0\n",
    "a = 'a'\n",
    "t = 't'\n",
    "qa_list = []\n",
    "while True:\n",
    "    page_number+=1\n",
    "    res = requests.get(main_url.format(page_number))\n",
    "    soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "    if \"No quotes found!\" in soup.select('.col-md-8')[1].getText():\n",
    "        print(\"reached final page ending program\")\n",
    "        break\n",
    "    for i in range(0,10):\n",
    "        if \"No quotes found!\" in soup.select('.col-md-8')[1].getText():\n",
    "            print(\"reached final page ending program\")\n",
    "            break\n",
    "        \n",
    "        a = soup.select('.author')[i].getText()\n",
    "        a = a.encode('ascii','ignore')\n",
    "        a = a.decode()\n",
    "        \n",
    "        \n",
    "        t= soup.select('.tags')[i].getText()\n",
    "        t = t.strip()\n",
    "        t = t.encode('ascii','ignore')\n",
    "        t = t.decode()\n",
    "        \n",
    "        csv_writer.writerow([a,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping details of different books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url =\"https://books.toscrape.com/catalogue/page-{}.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(main_url.format(50))\n",
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'404 Not Found' in soup.select('h1')[0].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = soup.select('.product_pod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "grabber = product[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frankenstein'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#title grabber\n",
    "#grabber = product[0]\n",
    "grabber.select('a')[1]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'38.00'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#price grabber\n",
    "grabber = product[0]\n",
    "a = grabber.select('p')[1].getText()\n",
    "a= a.encode('ascii','ignore')\n",
    "a = a.decode()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stock checker\n",
    "grabber = product[0]\n",
    "'In stock' in grabber.select('p')[2].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/frankenstein_20/index.html\n"
     ]
    }
   ],
   "source": [
    "#next url grabber\n",
    "next = grabber.select('a')[0]['href']\n",
    "sub_url = \"https://books.toscrape.com/catalogue/{}\"\n",
    "print(sub_url.format(next))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count of stock\n",
    "grabber = product[0]\n",
    "next = grabber.select('a')[0]['href']\n",
    "res2 = requests.get(sub_url.format(next))\n",
    "soup2 = bs4.BeautifulSoup(res2.text,\"lxml\")\n",
    "catch = soup2.select('table.table-striped')\n",
    "stock = catch[0]\n",
    "stock.select('td')[5].getText().split()[2][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating the Code to get Book title, Price and Stock quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('Book_Title_Price_Stock',mode = 'w',newline = '',encoding = 'utf-8')\n",
    "csv_writer = csv.writer(f,delimiter = ',')\n",
    "csv_writer.writerow(['Title','Price','Stock Available'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached final page ending program\n"
     ]
    }
   ],
   "source": [
    "page_number = 0\n",
    "main_url =\"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "sub_url = \"https://books.toscrape.com/catalogue/{}\"\n",
    "t = 'title'\n",
    "novel = []\n",
    "price = 'cost'\n",
    "cost = []\n",
    "maal = 'stock'\n",
    "stock = []\n",
    "instock=[]\n",
    "while True:\n",
    "    page_number+=1\n",
    "    res = requests.get(main_url.format(page_number))\n",
    "    soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "    if '404 Not Found' in soup.select('h1')[0].getText():\n",
    "        print(\"reached final page ending program\")\n",
    "        break\n",
    "    product = soup.select('.product_pod')\n",
    "    for i in range(0,20):\n",
    "        if '404 Not Found' in soup.select('h1')[0].getText():\n",
    "            print(\"reached final page ending program\")\n",
    "            break\n",
    "        grabber = product[i]\n",
    "        #grab title\n",
    "        t = grabber.select('a')[1]['title']\n",
    "        novel.append(t)\n",
    "        #grab price\n",
    "        a = grabber.select('p')[1].getText()\n",
    "        a= a.encode('ascii','ignore')\n",
    "        price = a.decode()\n",
    "        cost.append(price)\n",
    "        #check if in stoct\n",
    "        if 'In stock' in grabber.select('p')[2].getText():\n",
    "            link = grabber.select('a')[0]['href']\n",
    "            res2 = requests.get(sub_url.format(link))\n",
    "            soup2 = bs4.BeautifulSoup(res2.text,\"lxml\")\n",
    "            catch = soup2.select('table.table-striped')\n",
    "            stock = catch[0]\n",
    "            maal = stock.select('td')[5].getText().split()[2][1:]\n",
    "            instock.append(maal)\n",
    "        else:\n",
    "            maal = '0'\n",
    "            instock.append(maal)\n",
    "#         print(i)\n",
    "#         print(\"done\")\n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter data to file\n",
    "for i in range(len(novel)):\n",
    "    csv_writer.writerow([novel[i],cost[i],instock[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "#Book_Title_Price_Stock closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to differentiate books Tag wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = grabber.select('a')[0]['href']\n",
    "res2 = requests.get(sub_url.format(link))\n",
    "soup2 = bs4.BeautifulSoup(res2.text,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = soup2.select('table.table-striped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch[0].select('td')[5].getText().split()[2][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('book details',mode = 'w',newline = '',encoding = 'utf-8')\n",
    "csv_writer = csv.writer(f,delimiter = ',')\n",
    "csv_writer.writerow(['Title','Price','Stock Available'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('.nav.nav-list')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = \"https://books.toscrape.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(main_url)\n",
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_list = soup.select('.nav.nav-list')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classics'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_list.select('a')[5].getText().split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Travel'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_list.select('a')[1].getText().split()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catalogue/category/books/travel_2/index.html\n"
     ]
    }
   ],
   "source": [
    "link = whole_list.select('a')[1]['href']\n",
    "sub_url = \"https://books.toscrape.com/{}\"\n",
    "res2 = requests.get(sub_url.format(link))\n",
    "soup2 = bs4.BeautifulSoup(res2.text,\"lxml\")\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,000 Places to See Before You Die'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('.product_pod')[10].select('img')[0]['alt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup2.select('.product_pod'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = whole_list.select('a')[1]['href']\n",
    "sub_url = \"https://books.toscrape.com/{}\"\n",
    "res2 = requests.get(sub_url.format(link))\n",
    "soup2 = bs4.BeautifulSoup(res2.text,\"lxml\")\n",
    "for j in range(len(soup2.select('.product_pod'))):\n",
    "    books = []\n",
    "    titles = soup2.select('.product_pod')[j].select('img')[0]['alt']\n",
    "    books.append(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTUAL PERFECT PROGRAM FOR GETTING TAGS AND BOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('Book_Tag_BookName',mode = 'w',newline = '',encoding = 'utf-8')\n",
    "csv_writer = csv.writer(f,delimiter = ',')\n",
    "csv_writer.writerow(['Tag_Name','Novels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = []\n",
    "title = 'names'\n",
    "bknames = []\n",
    "main_url = \"https://books.toscrape.com\"\n",
    "res = requests.get(main_url)\n",
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "whole_list = soup.select('.nav.nav-list')[0]\n",
    "for i in range(len(whole_list.select('a'))):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    title = whole_list.select('a')[i].getText().split()[0]\n",
    "    tag.append(title)\n",
    "    link = whole_list.select('a')[i]['href']\n",
    "    sub_url = \"https://books.toscrape.com/{}\"\n",
    "    res2 = requests.get(sub_url.format(link))\n",
    "    soup2 = bs4.BeautifulSoup(res2.text,\"lxml\")\n",
    "    books = \"|\"\n",
    "    for j in range(len(soup2.select('.product_pod'))):\n",
    "        title = soup2.select('.product_pod')[j].select('img')[0]['alt']\n",
    "#         books.append(title)\n",
    "#         books.append('|')\n",
    "        books+=title\n",
    "    bknames.append(books)\n",
    "#writing to file\n",
    "for i in range(len(tag)):\n",
    "    csv_writer.writerow([tag[i],bknames[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
